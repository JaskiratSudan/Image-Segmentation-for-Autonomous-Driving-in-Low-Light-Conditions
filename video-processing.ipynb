{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10163291,"sourceType":"datasetVersion","datasetId":6276028},{"sourceId":10163672,"sourceType":"datasetVersion","datasetId":6276277},{"sourceId":10175024,"sourceType":"datasetVersion","datasetId":6284640},{"sourceId":10176080,"sourceType":"datasetVersion","datasetId":6285403},{"sourceId":194390,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":165706,"modelId":188040},{"sourceId":194395,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":165709,"modelId":188044},{"sourceId":195781,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":166929,"modelId":189248},{"sourceId":195783,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":166931,"modelId":189250}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install required libraries\n!apt install ffmpeg\n\n# Step 2: Import required libraries\nimport cv2\nimport imageio\nimport numpy as np\nimport tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:52.221638Z","iopub.execute_input":"2024-12-12T04:28:52.222112Z","iopub.status.idle":"2024-12-12T04:28:55.547332Z","shell.execute_reply.started":"2024-12-12T04:28:52.222073Z","shell.execute_reply":"2024-12-12T04:28:55.545866Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Step 3: Define video paths\ninput_video_path = '/kaggle/input/videos/0ae2db4ab2a7b56572db4ca3585fcba5.mp4'  \noutput_video_path = '/kaggle/working/BDD100k_output_video.mp4' ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:55.549704Z","iopub.execute_input":"2024-12-12T04:28:55.550336Z","iopub.status.idle":"2024-12-12T04:28:55.556045Z","shell.execute_reply.started":"2024-12-12T04:28:55.550282Z","shell.execute_reply":"2024-12-12T04:28:55.554881Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Step 4: Open the input video\ncap = cv2.VideoCapture(input_video_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:55.557504Z","iopub.execute_input":"2024-12-12T04:28:55.557909Z","iopub.status.idle":"2024-12-12T04:28:55.596549Z","shell.execute_reply.started":"2024-12-12T04:28:55.557874Z","shell.execute_reply":"2024-12-12T04:28:55.595578Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Check if the video file is successfully opened\nif not cap.isOpened():\n    print(\"Error: Unable to open video file.\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:55.598753Z","iopub.execute_input":"2024-12-12T04:28:55.599680Z","iopub.status.idle":"2024-12-12T04:28:55.604136Z","shell.execute_reply.started":"2024-12-12T04:28:55.599643Z","shell.execute_reply":"2024-12-12T04:28:55.603183Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Get video properties\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nprint(f\"Video Properties: Width={frame_width}, Height={frame_height}, FPS={fps}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:55.605304Z","iopub.execute_input":"2024-12-12T04:28:55.605614Z","iopub.status.idle":"2024-12-12T04:28:55.617825Z","shell.execute_reply.started":"2024-12-12T04:28:55.605583Z","shell.execute_reply":"2024-12-12T04:28:55.616661Z"}},"outputs":[{"name":"stdout","text":"Video Properties: Width=1920, Height=1080, FPS=25\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Step 5: Initialize the video writer\nwriter = imageio.get_writer(output_video_path, fps=fps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:55.619274Z","iopub.execute_input":"2024-12-12T04:28:55.619626Z","iopub.status.idle":"2024-12-12T04:28:55.638950Z","shell.execute_reply.started":"2024-12-12T04:28:55.619590Z","shell.execute_reply":"2024-12-12T04:28:55.637871Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\ndef load_pretrained_model(model_path):\n\n  def dice_loss(y_true, y_pred, smooth=1e-6):\n      y_true_f = tf.keras.backend.flatten(y_true)\n      y_pred_f = tf.keras.backend.flatten(y_pred)\n      intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n      dice = (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n      return 1 - dice\n\n  new_model = tf.keras.models.load_model(\n      model_path,\n      custom_objects={'dice_loss': dice_loss},\n      safe_mode=False  # Allow lambda loading\n  )\n\n  return new_model\n\n# Load the model with safe_mode=False\nmodel = load_pretrained_model('/kaggle/input/day_time_model/keras/default/1/Xception_unet_3.keras')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:55.640422Z","iopub.execute_input":"2024-12-12T04:28:55.640901Z","iopub.status.idle":"2024-12-12T04:28:57.025629Z","shell.execute_reply.started":"2024-12-12T04:28:55.640849Z","shell.execute_reply":"2024-12-12T04:28:57.024636Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Step 6: Define preprocess_frame function\ndef preprocess_frame(frame, target_size):\n    \"\"\"\n    Preprocesses a video frame to match the model's input size.\n    - Resizes the frame to target_size\n    - Normalizes pixel values to [0, 1]\n    - Adds a batch dimension\n    \"\"\"\n    frame_resized = cv2.resize(frame, target_size)  # Resize frame\n    frame_normalized = frame_resized / 255.0  # Normalize pixel values\n    return np.expand_dims(frame_normalized, axis=0)  # Add batch dimension\n\n# Define the input size expected by the model\ninput_size = (256, 256)  # Replace with your model's input dimensions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:57.027208Z","iopub.execute_input":"2024-12-12T04:28:57.027560Z","iopub.status.idle":"2024-12-12T04:28:57.033478Z","shell.execute_reply.started":"2024-12-12T04:28:57.027525Z","shell.execute_reply":"2024-12-12T04:28:57.032304Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Step 8: Process video frames in a loop\nwhile True:\n    # Read a frame from the video\n    ret, frame = cap.read()\n\n    # Break the loop if no frame is returned\n    if not ret:\n        break\n\n    # Preprocess the frame\n    preprocessed_frame = preprocess_frame(frame, input_size)\n\n    # Make a prediction\n    prediction = model.predict(preprocessed_frame)\n\n    # Ensure prediction is a NumPy array\n    prediction = np.array(prediction)  # Shape: (1, 256, 256, 1)\n\n    # Process the prediction: Aggregate or summarize\n    average_confidence = np.mean(prediction)  # Aggregate confidence\n    label = 1 if average_confidence > 0.5 else 0  # Example thresholding for binary classification\n\n    # Add text overlay with the prediction\n    text = f\"Class: {label}, Confidence: {average_confidence:.2f}\"\n    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Optionally, visualize the segmentation map (rescaled to frame size)\n    segmentation_map = (prediction[0] * 255).astype('uint8')  # Rescale to [0, 255]\n    segmentation_map = cv2.resize(segmentation_map, (frame.shape[1], frame.shape[0]))  # Match original frame size\n    overlay = cv2.addWeighted(frame, 0.7, cv2.cvtColor(segmentation_map, cv2.COLOR_GRAY2BGR), 0.3, 0)\n\n    # Write the frame with overlay to the output video\n    writer.append_data(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n\nprint(\"Video processing complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:57.034940Z","iopub.execute_input":"2024-12-12T04:28:57.035306Z","iopub.status.idle":"2024-12-12T04:28:59.467583Z","shell.execute_reply.started":"2024-12-12T04:28:57.035260Z","shell.execute_reply":"2024-12-12T04:28:59.465720Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     overlay \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maddWeighted(frame, \u001b[38;5;241m0.7\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mcvtColor(segmentation_map, cv2\u001b[38;5;241m.\u001b[39mCOLOR_GRAY2BGR), \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Write the frame with overlay to the output video\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert BGR to RGB\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo processing complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imageio/v2.py:226\u001b[0m, in \u001b[0;36mLegacyWriter.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV3 Plugins currently don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a uniform way to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m write metadata, so any metadata is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# total_meta = dict()\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# if meta is None:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m#     meta = {}\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# if hasattr(im, \"meta\") and isinstance(im.meta, dict):\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m#     total_meta.update(im.meta)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# total_meta.update(meta)\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_args\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imageio/plugins/tifffile_v3.py:224\u001b[0m, in \u001b[0;36mTifffilePlugin.write\u001b[0;34m(self, ndimage, is_batch, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     ndimage \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(ndimage)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m ndimage:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request\u001b[38;5;241m.\u001b[39m_uri_type \u001b[38;5;241m==\u001b[39m URI_BYTES:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh\u001b[38;5;241m.\u001b[39mclose()\n","\u001b[0;31mTypeError\u001b[0m: TiffWriter.write() got an unexpected keyword argument 'fps'"],"ename":"TypeError","evalue":"TiffWriter.write() got an unexpected keyword argument 'fps'","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"# Step 9: Release resources\ncap.release()\nwriter.close()\nprint(f\"Processed video saved at: {output_video_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:28:59.469020Z","iopub.status.idle":"2024-12-12T04:28:59.469619Z","shell.execute_reply.started":"2024-12-12T04:28:59.469333Z","shell.execute_reply":"2024-12-12T04:28:59.469362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}